{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Detecting the difficulty level of French texts](https://www.kaggle.com/c/detecting-the-difficulty-level-of-french-texts/overview/evaluation)\n",
    "### Implementing `Camembert` model\n",
    "---\n",
    "\n",
    "![camembertLogo](https://i2.wp.com/ledatascientist.com/wp-content/uploads/2020/11/camembert.png?resize=200%2C220&ssl=1)\n",
    "\n",
    "\n",
    "### What is Camenbert ?\n",
    "\n",
    "[Camembert](https://camembert-model.fr/) is a pre-trained NLP model. It is based on the [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/) architecture, a variant of BERT, released in 2019 by Facebook AI researchers. Camembert differs from other BERT-based models because it has been trained on a French corpus.\n",
    "\n",
    "\n",
    "### What is a pre-trained model ?\n",
    "\n",
    "The large amount of data and resources needed for many deep learning applications, and the large computation times for these, have encouraged researchers and data scientists to use pre-trained models. Such an approach is called transfer learning. It consists in reusing a model that has been trained on a specific task to another task similar to the first one.\n",
    "\n",
    "\n",
    "### How can it help us ?\n",
    "\n",
    "Words that have been seen by Camembert in the same context will have vector with close value (small distance). If we tell the model that a sentence with complex words is labelled as C1, there are chances that these words have been observed in a context close to other complicated words. If the model sees these, it will know that the vector is close to the vector that was labeled C1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fine-tuning the model\n",
    "\n",
    "This technic involved deeper technics so we won't go in all the details because that something we didn't see in class. But pretty much all everything we did is very well explained on huggingface website:\n",
    "\n",
    "https://huggingface.co/docs/transformers/training\n",
    "\n",
    "https://huggingface.co/camembert-base\n",
    "\n",
    "***Be careful***, the training takes a lot of time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Split dataset and encode labels\n",
    "We first need to split training data into a training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "Some layers of TFCamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import random as r\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from transformers import TFCamembertForSequenceClassification, CamembertTokenizer, AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# (for reproduciblity)\n",
    "r.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "training_data = 'https://raw.githubusercontent.com/LaCrazyTomato/Group-Project-DM-ML-2021/main/data/training_data.csv'\n",
    "\n",
    "df = pd.read_csv(training_data, encoding='utf-8')\n",
    "\n",
    "X = df['sentence'].values # to remove the index\n",
    "y = df['difficulty'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=0\n",
    "                                                  )\n",
    "\n",
    "# We need to encode output variable (since they are strings)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# We define the tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "config = AutoConfig.from_pretrained('camembert-base')\n",
    "config.num_labels = 6\n",
    "config.hidden_dropout_prob = 0.1\n",
    "\n",
    "model = TFCamembertForSequenceClassification.from_pretrained('camembert-base',\n",
    "                                                             config=config\n",
    "                                                             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 121, 11, 141, 744, 4669, 15, 13, 454, 4801, 12907, 9, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "inputs = tokenizer(\"J'étudie à la HEC Lausanne.\")\n",
    "\n",
    "encoded_sequence = inputs[\"input_ids\"]\n",
    "\n",
    "encoded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'étudie à la HEC Lausanne.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 121, 11, 141, 744, 4669, 15, 13, 454, 4801, 4802, 9, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"J'étudie à la HEC Genève.\")\n",
    "\n",
    "encoded_sequence = inputs[\"input_ids\"]\n",
    "\n",
    "encoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above an example of vectorization done by the camembert model.\n",
    "\n",
    "The input ids are often the only required parameters to be passed to the model as input. They are token indices, numerical representations of tokens building the sequences that will be used as input by the model (more infos [here](https://huggingface.co/docs/transformers/glossary))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fine-tune the model\n",
    "\n",
    "Now, what we will do, in simple words, is to show to the model which output value we attribute to which vectorized sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In deep learning, to train a model, we need a training set and a validation set\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_datasets_from_csv('training_data')\n",
    "\n",
    "# Tokenize\n",
    "X_train = tokenizer(X_train.tolist(), padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "X_train = {x: X_train[x] for x in tokenizer.model_input_names}\n",
    "X_val = tokenizer(X_val.tolist(), padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "X_val = {x: X_val[x] for x in tokenizer.model_input_names}\n",
    "X_test = tokenizer(X_test.tolist(), padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "X_test = {x: X_test[x] for x in tokenizer.model_input_names}\n",
    "\n",
    "\n",
    "# We will use tensorflow to build and train our model\n",
    "\n",
    "# SETTINGS\n",
    "BATCH_SIZE = 6\n",
    "EPOCHS = 6\n",
    "\n",
    "# Create tensorflow dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(BATCH_SIZE)\\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Prepare the model (solver, metrics)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(3e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# Train\n",
    "es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)\n",
    "lr_cb = tf.keras.callbacks.ReduceLROnPlateau(patience=0, restore_best_weights=True, verbose=1, min_lr=1e-8)\n",
    "csv_cb = tf.keras.callbacks.CSVLogger('history.csv')\n",
    "cp_cb = tf.keras.callbacks.ModelCheckpoint('cp')\n",
    "\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[es_cb, lr_cb, csv_cb, cp_cb]\n",
    "          )\n",
    "\n",
    "# Save the tuned model\n",
    "model.save_pretrained('trained_camembert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Predict with the model\n",
    "Now that we have trained our model , ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform([0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "All the layers of TFCamembertForSequenceClassification were initialized from the model checkpoint at trained_camembert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the trained model\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = TFCamembertForSequenceClassification.from_pretrained('trained_camembert')\n",
    "\n",
    "# 2) Tokenize the sentences\n",
    "example_sents = [\"Bob Seely, élu de l’île de Wight, s’est ainsi dit « fatigué » des « extrapolations ridicules » \\\n",
    "                    des scientifiques conseillant le gouvernement.\", \n",
    "                 \"Je mange une pomme.\", \n",
    "                 \"L'inflation inquiète la place financière.\"]\n",
    "\n",
    "tokenized_sents = tokenizer(example_sents, padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "model_input = {x: tokenized_sents[x] for x in tokenizer.model_input_names}\n",
    "\n",
    "\n",
    "model_output = model.predict(model_input) # Vector of the sentences\n",
    "pred_logits = model_output.logits # Probabilities\n",
    "pred_classes = np.argmax(pred_logits, axis=1) # Get the highest probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 4], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C1', 'A1', 'C1'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.inverse_transform(pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle unlabeled datas for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1195</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1196</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1197</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1198</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id difficulty\n",
       "0        0         C2\n",
       "1        1         A2\n",
       "2        2         B2\n",
       "3        3         A2\n",
       "4        4         C2\n",
       "...    ...        ...\n",
       "1195  1195         B1\n",
       "1196  1196         B1\n",
       "1197  1197         C2\n",
       "1198  1198         B2\n",
       "1199  1199         B2\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggleDf = pd.read_csv(\"https://raw.githubusercontent.com/LaCrazyTomato/Group-Project-DM-ML-2021/main/data/unlabelled_test_data.csv\")\n",
    "\n",
    "X = kaggleDf.sentence.to_list()\n",
    "\n",
    "\n",
    "tokenized_sents = tokenizer(X, padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "model_input = {x: tokenized_sents[x] for x in tokenizer.model_input_names}\n",
    "\n",
    "model_output = model.predict(model_input)\n",
    "pred_logits = model_output.logits\n",
    "pred_classes = np.argmax(pred_logits, axis=1)\n",
    "\n",
    "\n",
    "pred = pd.DataFrame({'id': [i for i in range(len(pred_classes))], 'difficulty': pred_classes})\n",
    "\n",
    "pred.difficulty = pred.difficulty.apply(lambda x: label_encoder.inverse_transform([x])[0])\n",
    "\n",
    "pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà ! That's how we achieved an accuracy of 57 % on the kaggle set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
